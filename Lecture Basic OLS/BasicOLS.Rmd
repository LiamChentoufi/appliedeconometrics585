---
title: "OLS Basics"
output: html_notebook
---
A good reference on basic econometrics with R can be found on [this](https://www.econometrics-with-r.org/index.html) website, where you can also do some practice exercises with R directly.

We'll be importing excel datasets, so will need to install a package to do so:

```{r}
#install.packages("readxl")
```

Now let's load the library and import the dataset
```{r}
library("readxl")
ceodata <- read_excel("/Users/mkaltenberg/Dropbox/Pace/ECO585/R notebooks/Data/excelfiles/ceosal1.xls", col_names = c("salary", "pcsalary", "sales", "roe","pcroe", "ros","indus","finance","consprod","utility","lsalary", "lsales"))
```

Let's see how to rename variables
```{r}

library(tidyverse)
ceosalary<- as_tibble(ceodata)

#an example on how to rename variables
ceosalaryex <- ceosalary %>% 
   rename(inc = salary, returnexp = roe)

```
Let's estimate a simple regression:

\begin{gather*}\widehat{s a l a r y} =\beta_0 +\beta_1\text{}r o e \end{gather*}

where annual CEO salary is in thousands of dollars and the return on equity is a percent.

Let's run that regression in R:
```{r}
library(wooldridge)
data("ceosal1")
roe_reg <- lm(salary ~ roe, data = ceosal1)
summary_roe <-summary(roe_reg)
summary_roe
```

How do you interpret this coefficient?

A one percentage point increase in $roe$ increases predicted salary by $18.501$, or $18,501

Remember that we are trying to understand differences in one unit. A change in one unity in roe is percentage points. Percent change is a ratio - it measures a rate of change. Take this example, the percent of people on poverty in the USA in 2015 was 13.5% and in 2020 it is 11.3%. The the poverty rate between 2015 and 2020 is -2.2 percentage points.  However, the percent change of poverty between 2015 and 2020 is -12.6% ($\frac{13.5-11.3}{13.5}$). The poverty rate decreased by 12.6%. Percent is a number expressed as a fraction of 100. So, the percent of people in poverty is a reflection of $\frac{N. Poverty}{Tot. Population}$.

Take this example: 

What if we measure $roe$ as a decimal, rather than a percent? Define 

\begin{equation*}r o e d e c =\frac{r o e}{100}
\end{equation*}

What will happen to the intercept, slope, and $R^{2}$ when we regress 

\begin{equation*}s a l a r y\quad \text{on}\quad r o e d e c\text{?}
\end{equation*}

Nothing should happen to the intercept: $roedec =0$ is the same as $r o e =0$. But the slope will increase by 100. The goodness-of-fit should not change, and it does not.

```{r}
ceosal1 <- ceosal1 %>%
  mutate(roedec= roe/100)

roedec<-lm(salary ~ roedec, data =ceosal1)

roedec_sum <- summary(roedec)
roedec_sum
```
Now a one percentage point change in $roe$ is the same as $\Delta r o e d e c =.01$, and so we get the same effect as before.

What if we measure salary in dollars, rather than thousands of dollars? 

Both the intercept and slope get multiplied by 1,000


# OLS properties
Let's see some of the OLS properties in action

1) The sum of residuals will equal to 0
Let's see it:
```{r}
u <- resid(lm(salary ~ roedec, data =ceosal1))
sum(u)
```
There are some rounding errors in computation, but the sum is close to 0

2) We know that $\bar{y}=\bar{\hat{y}}$

Let's show it:
```{r}
yhat <- predict(lm(salary ~ roedec, data =ceosal1))

mean(ceosal1$salary)
mean(yhat)
```

The sample covariance (and therefore the sample correlation) between the explanatory variables and the residuals is always zero:

```{r}
sum(ceosal1$roedec*u)
```
Again, there are some rounding errors in computation, but the sum is close to 0


Because the $\hat{y}_i$ are linear functions of the $x_i$, the fitted values and residuals are uncorrelated, too:
```{r}
sum(yhat*u)
```
Again, there are some rounding errors in computation, but the sum is close to 0

3) The point ($\hat{x}, \hat{y}$) is always on the OLS regression line. That is, if we plug in the average for x, we predict the sample average for y:
$\bar{y} = \beta_0 + \beta_1\bar{x}̄$

```{r}
mean(ceosal1$roedec)
mean(ceosal1$salary)
```
We can plot our regression 


And we can graph our residuals like this:
```{r}
plot(salary ~ roedec, data=ceosal1)
abline(lm(salary ~ roedec, data =ceosal1))
points(mean(ceosal1$roedec), mean(ceosal1$salary), col = "red")
```

## Simple Linear Regression Assumptions

Recall that for the Simple Linear Regression the assumptions are:
  
  1. Linear in parameters. x and u are random, and thus, so is y.
  
  2. Random sample size of n
  
  3. Sample Variation in the Explanatory Variable (you need to have variance in X - it can't be a constant)
  
  4. Zero Conditional Mean (error term has zero mean given any value of the explanatory variable) $E(u|x)=0$ for all x
  
  5. Homoskedasticity or Constant Variance (error term has the same variance given any value of the explanatory variable x, $Var(u|x) = \delta^2 > 0$ for all x

We mostly work in the realm of multiple linear regressions and the difference between the simple linear regression and multiple linear regressions requires only one additional assumption, so we'll just hop right into it.

## Multiple Linear Regression Assumptions

1. $y=\beta_0 + \beta_1x_1+\beta_2x_2 + \dots + \beta_kx_k + u$ (linearity)

2. Random sampling from the population

${(u_i,X_i), i =1, \dots, N}$ is a random sample from the $(u, X)$ population where  $(u_i, X_i)$ are i.i.d. (independently and identically  distributed)

3. no perfect collinearity in the sample (aka non redundancy)

$\sum^n_{i=1} (X_i-\bar{X})^2>0$

4. $E(u|x_1,\dots,x_k) = E(u) = 0$ (zero conditional mean)

5. $Var(u|x_1, \dots, x_k) = Var(u) = \sigma^2$ (homoskedasticity)

6. $u_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(0,\sigma^2_u)$
The distribution u given $(x_1, \dots, x_k$ is $Normal(0,\sigma^2)$  (Normality of the error term)

Assumptions 1 - 4 ensure unbiasedness in our parameter estimation aka $E(\hat{\beta_j})=\beta_j$

Assumptions 1-5 are the Gauss-Markov assumptions aka BLUE

Assumption 1-6 are the classic linear assumptions and do exact statistical inference.

Sometimes, you'll see the conditional expectation function written as:

$E(Y_i|X_i) = \beta_0 + \beta_1X_i$

This implies:

i. The conditional expectation of $Y_i$ is linear in $X_i$ (Asmp. 1)
ii. the error term is mean-independent of the regressor (Asmp. 4)
iii. the error term is distribution around zero.
(Asmp. 5)

Assumptions 1, 5 and 6 are easier to relax. Assumption 3 is easy to implement and Assumption 2 is data dependent, but will be important for panel data.
Assumption 4 - which we've discussed a lot, is what a lot of econometrics is all about. 

We can write assumptions 1,3,4,5 concisely:

$Y_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(\beta_0 +\beta_1X_i, \sigma^2)$

or

$u_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2_u)$


### Perfect Collinearity (Assumption 3)
Assumptions 1,2, 4, and 5 are the same as in the simple linear model. The new assumption is about perfect collinearity. When estimating OLS, perfect linear combinations can't be distinguished. In other words, we can't distinguish the estimates from the two variables and there are infinite possible answers. 

This seems obvious in cases where we have a set of categorical dummies. The solution to this issue is to drop a variable, and as consequence, our interpretation changes. We interpret a set of dummies relative to the one we dropped.  

Take this example, where we measure the impact of region on wages. We interpret the coefficient of south relative the variable excluded (east). Including all variables would lead to perfect collinearity.


```{r}
#install.packages("wooldridge")
library(wooldridge)
data("wage1")

dummies <- lm(lwage ~ educ+tenure+smsa+northcen+south+west, data= wage1)

summary(dummies)
```

The problem gets harder if for some reason you have perfect linear combinations of other variables (that you didn't realize). This is why it's good to do correlation matrices at the start of your research, so you can see if there are variables that are perfectly correlated (and also to check for severity of multi-collinearity, which we will get to later).

Consider non-obvious example - imagine that you have information about wages, industry and union rates for Sweden. You want to understand how industry and union participation impacts average earnings $ln(wage_i) = \beta_0 + \beta_1 Union_i + \sum_j^i\beta_j Ind_i +u_i$ where j represents the number of industry dummies and union is a dummy variable on whether an individual has a union contract or not. If one industry, let's say manufacturing, has every employee under a union contract, this will result in perfect collinearity. This is because Industry Manuf and Union are perfectly collinear - OLS can't solve the estimate for manuf industry and union as there are infinite number of solutions. This example is something where you will have to drop either the industry manuf. or union dummy. 

So, we see what happens when assumption 3 breaks. R may automatically drop a perfectly collinear variable, but if it drops without you realizing why, you should review your data. In practice, this is something that you may make a mistake and R fixes it for you, but assumptions 4 and 5 are much more nuanced. Assumption 5 is an assumption we can often "relax" by correcting our standard errors. Assumption 4 is the trickiest assumption to fix, and one that econometricians and applied econometricians spend a lot of time thinking about. 

As a side note, assumption 1 is also something that econometricians work on - particularly when parameters can behave non-linearly or when the dependent variable has a non-normal distribution. 

### Zero-conditional mean (Assumption 4)

This assumption is by far the most important assumption - if this is broken, then we have a problem with biasedness. Bad news is that this assumption is broken frequently. 

It is the glue of showing causality.

It's easiest to think of this in a randomized setting or a randomized control trial. 

We want to evaluate the effectiveness of health care on health outcomes. In an ideal world, we'd create two groups of identical people. Where we'd have one twin group that is exposed to health care and the other twin group that does not have health care - and this assignment is *random*. Then we'd compare their health outcome differences - a difference in the means of the two groups. 

Obviously, we don't have identical twins in every possible way roaming around. So, the best we an do is a build an ideal setting where we have nearly identical groups.  Any setting in which the two groups that we are comparing are non-randomly different are what makes up our error term. 

So, you can imagine, that there are tons of settings in which the groups we may differ - and those situations will break assumption 4. 

Econometrics is mostly building an statistically ideal situation in which we have two identical groups and the only difference between the two groups are randomly assigned - that is your X and a variable that you are interested in understanding. 

And when you consider your research, your interest should focus on a couple of X's (meaning that you want to create an ideal setting for a couple of variables - you want to understand unbiased estimates for a one or a couple variables).  The other variables that you include - those controls - are meant to help you create this ideal setting, but often, they can biased (and that's ok - so long as you understand the caveats and you don't interpret those coefficients as causal).

### Unbiasedness of Beta 
The key assumption that you must think about the most is if assumption 4 is met. This assumption is often broken, and as a consequence will result in biasedness in your $\beta$ parameters.

Technically, assumptions 1-4 are required for unbiasedness, but in practical terms, often assumption 4 is biggest concern.  When assumption 1 is broken, we use non-linear estimation techniques that's outside of the scope of this course. Assumption 2 is a concern when you choose your data (target population), complex survey designs, designing randomized control trials, and time series data (we'll talk about this later with panel data).

We can do an experiment to see if $\beta_0$ and $\beta_1$ are unbiased when assumptions 1-4 hold. 

We will randomly draw from a normal distribution for u and x. Then we will create a y variable as a combination of x and u, specifically $y=3+2x+u$.This way will know that $\beta_0$ should be 3 and $\beta_1$ should be 2. We'll repeat this process multiple times and will see that the estimates should be close to our expected values.

```{r}
x = 3*rnorm(250)
u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)
```
You can see that the values are note exact, but they are close to expected values $\beta_1=2, \beta_0=3$. If we average these values after many random samples, we should get the true population estimate.

We won't ever know the true population value, but we hope that our sample is “typical” and produces a slope estimate close to $\beta_1$, but we can never know. 


### Omitted Variable Bias
There are many ways to break assumption 4, omitted variable bias is just one example - so, let's dive into this particular violation of assumption 4.

#### Theory 

We require two conditions for the coefficient on $X_1$ to "suffer" from omitted variable bias.

1. $X_1$ is correlated with the omitted variable ($X_2$)
2. The omitted variable ($X_2$) is a determinant of the outcome ($Y$)

If omitted variable bias is present then $\hat{\beta}_1$ does not converge (in probability) to the true value but instead to the following:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \rho_{X_1 u} \frac{\sigma_u}{\sigma_{X_1}}$$
Where $\rho_{X_1 u}$ is the correlation between the error term ($u$) and $X_1$, $\sigma_u$ is the std dev of the error term and $\sigma_{x_1}$ is the std dev of $x_1$
Quite generally, this means that there is something in the error term ($u$) that is correlated with both $X_1$ and $Y$, which invalidates the zero conditional mean assumption and biases the coefficient of $X_1$ in a particular direction.

#### Deriving the formula

The OLS estimator of the slope in a bi-variate regression:

$$ \hat{\beta_1} = \beta_1 + \frac{\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)u_i}{\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)^2} $$

The nominator of the second term converges in probability to the covariance of $X_i$ and $u_i$, so that $\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)u_i \buildrel p \over \longrightarrow  \text{cov}(X_{1i}, u_i) = \rho_{X_1 u} \sigma_u \sigma_{X_1}$. The denominator converges in probability to the variance of $X_{1i}$, so that $\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)^2 \buildrel p \over \longrightarrow \text{var}(X_{1i}) = \sigma_{X_1}^2$. As a result, the entire equation converges to

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\text{cov}(X_{1i}, u_i)}{\text{var}(X_{1i})} $$
Or, written differently, from reshuffling the definition of a correlation, i.e. $\rho_{X_{1} u} = \text{cov}(X_{1i},u) / (\sigma_{X_1} \sigma_u)$, we have:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\rho_{X_1 u} \sigma_u \sigma_{X_1}}{\sigma_{X_1}^2} = \beta_1 + \rho_{X_1 u}\frac{ \sigma_u}{\sigma_{X_1}} $$ where the last equality follows simply from canceling terms.

If the OLS assumptions are satisfied, the last term becomes zero and our OLS estimator of $\beta_1$ is unbiased and consistent. However, if there are important omitted variables, then the OLS of $\beta_1$ is biased and inconsistent (i.e. even with large N the bias does not vanish).


#### Deriving the direction (formally)

Suppose you have two models, one simple bivariate OLS (aka the "wrong" model) 
$$ Y_i = \beta_0 + \beta_1 X_{1i} + u_i $$
and the "true" model, in which two X's are included
$$ Y_i = \gamma_0 + \gamma_1 X_{1i} + \gamma_2 X_{2i} + e_i $$
We want to know the direction of the bias in $\hat{\beta}_1$ based on the first (wrong) model.

If we look at the formula for the OV-bias, we cannot directly compute $\rho_{X_1 u}$ -- the correlation between $X_{1i}$ and the error ($u_i$) -- because the population error is not observed. However, we can clearly see that the "true" model is different from the biased model in terms of parameters and in the sense that $u_i = \gamma_2 X_{2i} + e_i$. Further, since the second model is the true model, we have $\text{E}[e_i | X_{1i}, X_{2i}]=0$; that is, the expectation of $e_i$ will be zero and the error is not systematically correlated with $X_{1i}$.

Now we can rewrite the covariance between $X_{1i}$ and the error term ($u_i$) in terms of observables using the alternative formulation from above for simplicity:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\text{cov}(X_{1i}, u_i)}{\text{var}(X_{1i})} = \beta_1 + \frac{\text{cov}(X_{1i}, [\gamma_2 X_{2i} + e_i])}{\text{var}(X_{1i})} $$

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\gamma_2\text{cov}(X_{1i}, X_{2i}) + \text{cov}(X_{1i}, e_i) }{\text{var}(X_{1i})} =  \beta_1 + \frac{\gamma_2\text{cov}(X_{1i}, X_{2i}) + 0 }{\text{var}(X_{1i})}$$

which leads us to the last version[^2]

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 +  \frac{\gamma_2\text{cov}(X_{1i}, X_{2i})}{\text{var}(X_{1i})} = \beta_1 + \gamma_2 \rho_{X_{1} X_{2}} \frac{\sigma_{X_2}}{\sigma_{X_1}} $$

We do not need to know the ratio of the two standard deviations ($\frac{\sigma_{X_2}}{\sigma_{X_1}}$) to derive the direction, as it will always be positive, so all we care about is $\gamma_2 \rho_{X_{1} X_{2}}$. We can simply estimate $\gamma_2$ by running the second model and similarly just estimate the correlation $\rho_{X_{1} X_{2}}$. All that's left is to figure out the direction of the true coefficient. We can determine the sign of the "true" $\beta_1$  by estimating its counterpart $\gamma_1$. Then we have

1. if $\beta_1 < 0$ and $\gamma_2 \rho_{X_{1} X_{2}}< 0$, then $\beta_1$ is downward biased and overestimated.
2. if $\beta_1 < 0$ and $\gamma_2 \rho_{X_{1} X_{2}}> 0$, then $\beta_1$ is upward biased and underestimated.
3. if $\beta_1 > 0$ and $\gamma_2 \rho_{X_{1} X_{2}}< 0$, then $\beta_1$ is downward biased and underestimated.
4. if $\beta_1 > 0$ and $\gamma_2 \rho_{X_{1} X_{2}} > 0$, then $\beta_1$ is upward biased and overestimated.

A simple way to remember this is:

|     | $Corr(x_1,x_2)>0$ | $Corr(x_1,x_2)<0$ |
|-----|-------------------|--------------------|
| $Corr(y,x) > 0$| upward bias | downward bias | 
| $Corr(y,x) < 0$| downward bias | upward bias | 

This works well only if we compare a single covariate model to a model with one additional X. In practice, there may be multiple omitted variables at work, each possibly biasing the true coefficient of $X_1$ in different directions. Theoretically, we can even envision a case where there are two omitted variables in opposing directions with offsetting effects!

[^2]:In the last equality we use the same trick as applied by S&W when deriving the original formula. Note that $\rho_{X_{1} X_{2}} = \text{cov}(X_{1i},X_{2i}) / (\sigma_{X_1} \sigma_{X_2})$. Reshuffling and then canceling terms gives the last expression.

Now, let's take a practical example of Omitted Variable Biasedness.

There is a package that contains all of Wooldridge's data, so let's make our life easy:
```{r}
#install.packages("wooldridge")
```

```{r}
library(wooldridge)
```

Load in the education data (you can load data into R from the package directly)
```{r}
wagedata <-data("wage1")
biased <- lm(lwage ~ educ +tenure, data= wage1)
summary(biased)

less_biased <-lm(lwage ~ educ+exper+tenure, data= wage1)
summary(less_biased)
```

We can see that the coefficient of education changes when we compare our regression with (.0865) and without experience (.097).  It increases by ~10% - meaning that without the inclusion of experience, education was downwardly biased. 

### A note about Adjusted R-squared

The Sum of Squares decomposition still holds with $K$ regressors \vspace{-1em}

$$ \sum_{i=1}^N (Y_i - \bar{Y})^2 = \sum_{i=1}^N (\hat{Y}_i - \bar{Y})^2 + \sum_{i=1}^N \hat{u}_i^2 $$ 

However, the $R^2$ keeps going up when we add (even meaningless) regressors. So we introduce a penalty  \vspace{-1em}

This is the Adjusted $R^2$
$$\bar{R}^2 \equiv 1 - \left(\frac{N-1}{N-K-1}\right) \frac{\sum_{i=1}^N \hat{u}_i^2}{\sum_{i=1}^N (Y_i - \bar{Y})^2}$$

Similarly, the $SER$ now needs to be adjusted for the extra degrees-of-freedom used during the estimation \vspace{-1em}

$$SER \equiv s_{\hat{u}} = \sqrt{\frac{1}{N-K-1}\sum_{i=1}^N \hat{u}_i^2}$$
Can the adjusted $R^2$ decrease if we include a new variable? Depends if it is relevant or not.

What happens if we include an irrelevant variable? Let's see an example...

```{r}
mlb <-lm(lsalary ~years + gamesyr+bavg + hrunsyr+rbisyr, data = mlb1)

mlb2 <-lm(lsalary ~years + gamesyr+bavg + hrunsyr+rbisyr+percwhte, data = mlb1)
summary(mlb)
summary(mlb2)


```

When more regressors are added, SSR falls, but so does $d f =n -k -1$. $\bar{R}^{2}$ can increase or decrease. 

For $k \geq 1$, $\bar{R}^{2} <R^{2}$ unless $S S R =0$ (not an interesting case). In addition, it is possible that $\bar{R}^{2} <0$, especially if $d f$ is small. Remember that $R^{2} \geq 0$ always.

More algebraic facts: 

1. If a single variable is added to a regression, $\bar{R}^{2}$ increases if and only if the absolute $t$ statistic of the new variable is greater than one. (Note this value does not correspond to any commonly used critical value.)


2. If two or more variables are added to a regression, $\bar{R}^{2}$ increases if and only if the $F$ statistic for joint significance of the new variables is greater than one. (Again, the value one is a curiosity; it does not correspond to commonly used critical values for $F$ tests.)

Sometimes useful to write:

\begin{equation*}\bar{R}^{2} =1 -(1 -R^{2}) \frac{(n -1)}{(n -k -1)}
\end{equation*}

Important: In the $R$-squared form of the $F$ statistic that we covered, it is the usual $R$-squared, not the adjusted $R$-squared, that appears. 

Sometimes $\bar{R}^{2}$ is called the ``corrected $R$-squared,'' but this name is problematic. It implies that $\bar{R}^{2}$ corrects some statistical deficiency that $\bar{R}^{2}$ has for estimating $\rho ^{2}$. But like $R^{2}$, $\bar{R}^{2}$ is biased for $\rho ^{2}$.


### Using Adj. R-squared to choose between models

Typically, if you tell me that you chose one model over the other because it has a higher Adj. R-square, I'm going to be *VERY* skeptical. Blindly using R-square to determine the "best" model is a big no-no. 

However, there are situations where it may be helpful.

When you are trying to figure out which proxy is better than another, it might be useful.

OR maybe you want to consider different functional forms (Do I use a log or a polynomial? - both represent diminishing returns)

This is what wooldridge calls comparing *nonnested models*.

NOTE that it is only possible to do these comparisons when the dependent model remains the same. You can't use adjusted R-square to figure out if log(y) or y is the better option. 








